---
title: "R Notebook"
output: html_notebook
---

# Chapter 13 Probability

Today probability theory is being used much more broadly with the word _probability_ commonly used in everyday language.

Google’s auto-complete of “What are the chances of” give us: “having twins”, “rain today”, “getting struck by lightning”, and “getting cancer”. One of the goals of this part of the book is to help us understand how probability is useful to understand and describe real-world events when performing data analysis.

Because knowing how to compute probabilities gives you an edge in games of chance, throughout history many smart individuals spent time and energy thinking through the math of these games. As a result, Probability Theory was born. Probability continues to be highly useful in modern games of chance. For example, in poker, we can compute the probability of winning a hand based on the cards on the table.

Probability theory is useful in many other contexts and, in particular, in areas that depend on data affected by chance in some way. All of the other chapters in this part build upon probability theory. Knowledge of probability is therefore indispensable for data science.



## 13.1 Discrete probability

We start by covering some basic principles related to categorical data. The subset of probability is referred to as _discrete probability_. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples.



### 13.1.1 Relative frequency

Here we discuss a mathematical definition of _probability_ that does permit us to give precise answers to certain questions.

For example, if I have 2 red beads and 3 blue beads inside an urn47 (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue.

A more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.



### 13.1.2 Notation

We use the notation $Pr(A)$ to denote the probability of event $A$ happening. We use the very general term _event_ to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.

In data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: $X≥6$. We will see more of these examples later. Here we focus on categorical data.



### 13.1.3 Probability distributions

If we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.
확률분포?

If we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group. The probability distribution is:
```
Pr(picking a Republican)	=	0.44
Pr(picking a Democrat)	=	0.44
Pr(picking an undecided)	=	0.10
Pr(picking a Green)	=	0.02
```



## 13.2 Monte Carlo simulations for categorical data

Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.

An example is the `sample` function in R. We demonstrate its use in the code below. First, we use the function `rep` to generate the urn:

```{r}
beads <- rep(c("red", "blue"), times = c(2,3))
beads
#> [1] "red"  "red"  "blue" "blue" "blue"
```

and then use `sample` to pick a bead at random:

```{r}
sample(beads, 1)
```

This line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. **This is an example of a _Monte Carlo_ simulation**.

Much of what mathematical and theoretical statisticians study, which we do not cover in this book, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this section, we provide a practical approach to deciding what is “large enough”.

To perform our first Monte Carlo simulation, we use the `replicate` function, which permits us to repeat the same task any number of times. Here, we repeat the random event $B=$10,000 times:

```{r}
B <- 10000
events <- replicate(B, sample(beads, 1))
```

We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use `table` to see the distribution:

```{r}
tab <- table(events)
tab
```

and `prop.table` gives us the proportions:

```{r}
prop.table(tab)
```

Although this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.



### 13.2.1 Setting the random seed

Before we continue, we will briefly explain the following important line of code:

```{r}
set.seed(1986) 
```

Throughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the book may show a different result than what you obtain when you try to code as shown in the book.
할 때마다 다른 결과 뜰 것.

This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number.
특정 숫자로 고정?

Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018:
$2018 - 12 - 20 = 1986$

You can learn more about setting the seed by looking at the documentation:

```{r}
?set.seed
```

In the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.



### 13.2.2 With and without replacement

The function `sample` has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs _without replacement_: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:

```{r}
sample(beads, 5)
sample(beads, 5)
sample(beads, 5)
```

This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:

```{r}
sample(beads, 6)
```

`Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE'`

However, the `sample` function can be used directly, without the use of `replicate`, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell `sample` to do this by changing the replace argument, which defaults to `FALSE`, to `replace = TRUE`:

```{r}
events <- sample(beads, B, replace = TRUE)
prop.table(table(events))
```

Not surprisingly, we get results very similar to those previously obtained with `replicate`.



## 13.3 Independence

We say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.
replace~돌려놓다 인 듯

Many examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, $...$, Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent: the first outcome affected the next one.

To see an extreme case of non-independent events, consider our example of drawing five beads at random **without** replacement:

```{r}
x <- sample(beads, 5)
```

If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:

```{r}
x[2:5]
#> [1] "blue" "blue" "blue" "red"
```
뽑을 수록 확률이 변해서. 이제 red하나만 남았으니 red 뽑을 확률이 1.

would you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.



## 13.4 Conditional probabilities

When events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:

$Pr(Card 2 is a king∣Card 1 is a king)=3/51$

We use the $∣$as shorthand for “given that” or “conditional on".

When two events, say $A$ and $B$, are independent, we have:

$Pr(A∣B)=Pr(A)$
A and B 라고 했는데도 확률이 P(A)에서 낮아지거나 높아지지 않았음.

This is the mathematical way of saying: the fact that $B$ happened does not affect the probability of $A$ happening. In fact, this can be considered the mathematical definition of independence.



## 13.5 Addition and multiplication rules

### 13.5.1 Multiplication rule

If we want to know the probability of two events, say $A$ and $B$, occurring, we can use the multiplication rule:

$Pr(A and B)=Pr(A)Pr(B∣A)$

Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).

So, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: $1/13×16/51≈0.025$

The multiplication rule also applies to more than two events. We can use induction to expand for more events:

$Pr(A and B and C)=Pr(A)Pr(B∣A)Pr(C∣A and B)$



### 13.5.2 Multiplication rule under independence

When we have independent events, then the multiplication rule becomes simpler:

$Pr(A and B and C)=Pr(A)Pr(B)Pr(C)$

But we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.

As an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only $1/10×1/5$ or 0.02 have both.

But to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95.
mustache 가진 사람이 beard가질 확률이 0.95다?

So the correct calculation probability is much higher: $1/10×95/100=0.095$.
10분의 1의 사람 중 95%.. (beard가진 사람 중의 mustache가진 사람은? 그것도 0.095인가?) (아니 여기서는 0.2곱하기 뭐여..)

The multiplication rule also gives us a general formula for computing conditional probabilities:

$Pr(B∣A)=Pr(A and B)/Pr(A)$

To illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.



### 13.5.3 Addition rule

The addition rule tells us that:

$Pr(A or B)=Pr(A)+Pr(B)−Pr(A and B)$

This rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance.

(벤다이어그램 그림)



## 13.6 Combinations and permutations

In our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.

For more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.
다시 집어넣지 않았다.

First, let’s construct a deck of cards. For this, we will use the `expand.grid` and `paste` functions. We use `paste` to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:

```{r}
number <- "Three"
suit <- "Hearts"
paste(number, suit)
class(paste(number, suit))
#> [1] "Three Hearts"
```

`paste` also works on pairs of vectors performing the operation element-wise:

```{r}
paste(letters[1:5], as.character(1:5))
#> [1] "a 1" "b 2" "c 3" "d 4" "e 5"
```
벡터도 순서대로 붙여준다.

```{r}
paste(letters[1:5], as.character(1:7))
```
숫자가 안 맞으면 적은 숫자인 걸 다시 돌리나 봄.

The function `expand.grid` gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:

```{r}
expand.grid(pants = c("blue", "black"), shirt = c("white", "grey", "plaid"))
```
모든 조합을 준다고? 단순히 수 적은 걸 여러 번 돌리는 게 아니고? 아.. 그렇네. paste와의 차이인가? 그렇군. 그리고 이건.. data frame이군.

Here is how we generate a deck of cards:

```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", 
             "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number=numbers, suit=suits)
deck <- paste(deck$number, deck$suit)
```
아아.. expand.grid로 모든 조합 만들어 둔 후, column끼리 이어 붙여 스트링으로 만들었구나.

With the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:

```{r}
kings <- paste("King", suits)
mean(deck %in% kings)
```
1과 0이 52개 있는데 그것의 mean이라는 건 52분의 (1개수 곱하기 1) 같은 건가.

Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.
이미 킹 하나 뽑았으니 51개 카드 남았고 그 안에 킹은 3개 남았지.

To do this, we can use the `permutations` function from the gtools package. For any list of size `n`, this function computes all the different combinations we can get when we select `r` items. Here are all the ways we can choose two numbers from a list consisting of `1,2,3`:

```{r}
library(gtools)
permutations(3, 2)
```
사이즈 3인 그룹에서 2개 조합을 순서 있게 뽑은 듯? 숫자만 되나?
아, 그런데 하나 뽑고 나면 그 다음에 다시 집어넣지 않은 것으로 한 듯.

Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.
같은 게 다시 뽑히지 않는구만.

Optionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:

```{r}
all_phone_numbers <- permutations(10, 7, v = 0:9)
n <- nrow(all_phone_numbers)
index <- sample(n, 5)
all_phone_numbers[index,]
```
v라는 세 번째 argument로 뭔가 지정해 줄 수 있나 봄? input같은 건가..
모든 휴대폰 번호의 조합 604800개 중 5개를 임의로 뽑고. 그 row번호는 index에.
그리고 all_phone_numbers에서 index의 row 번호 가진 녀석 뽑았군.

Instead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.

To compute all possible ways we can choose two cards when the order matters, we type:

```{r}
hands <- permutations(52, 2, v = deck)
```
역시 숫자가 디폴트일 뿐, 캐릭터도 받아들이는군. input data가 몇 개인지 자동적으로 받아들여서 n 위치에 넣으면 안 되는 건가..? 흠.. 뭐지?

This is a matrix with two columns and 2652 rows. With a matrix we can get the first and second cards like this:

```{r}
first_card <- hands[,1]
second_card <- hands[,2]
```

Now the cases for which the first hand was a King can be computed like this:

```{r}
kings <- paste("King", suits)
sum(first_card %in% kings)
#> [1] 204
```
첫 번째로 뽑은 카드가 King이었던 경우의 수가 204!

To get the conditional probability, we compute what fraction of these have a King in the second card:

```{r}
sum(first_card%in%kings & second_card%in%kings) / sum(first_card%in%kings)
#> [1] 0.0588
```
첫 번재 카드가 king일 때.. 두 번째 카드도 king일 확률.

which is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:

```{r}
mean(first_card%in%kings & second_card%in%kings) / mean(first_card%in%kings)
#> [1] 0.0588
```

which uses `mean` instead of `sum` and is an R version of:
$Pr(A and B)Pr(A)$

sum과 mean의 차이는.. sum은 분모가 생략되어 있다. mean은 분모로 총 개수인 kings의 개수가 들어 있는데.

How about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a _Natural 21_ and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.
순서가 상관없을 때.
그러니까, 위의 경우는 첫 번째에 꼭 킹을 뽑아야 해서 순서가 있던 건데 / 아래쪽의 경우는 에이스를 앞에 뽑든 뒤에 뽑든 같은 결과가 나오니 순서가 상관 없다는 건가?

```{r}
combinations(3,2)
```
조합. 순서 상관없이 단지 묶음으로 뽑혔군?
permutation은 순서 있었는데, combination은 순서 없군.

In the second line, the outcome does not include (2,1) because (1,2) already was enumerated.

So to compute the probability of a _Natural 21_ in Blackjack, we can do this:

```{r}
aces <- paste("Ace", suits)
#Ace인 것들 뽑고.

facecard <- c("King", "Queen", "Jack", "Ten")
#facecard 4종류 저장.
facecard <- expand.grid(number = facecard, suit = suits)
#facecard와 suit종류의 모든 조합. 4*4 = 16조합.
facecard <- paste(facecard$number, facecard$suit)
#facecard와 suit를 이어 붙여 하나의 스트링으로 만든다.

hands <- combinations(52, 2, v = deck)
#순서 상관없이, 단지 조합. (A,B)의 경우 A와 B가 함께 존재하는 단 하나의 조합만 기록.
mean(hands[,1] %in% aces & hands[,2] %in% facecard)
#(첫번째 column 중에서 ace인 것임과 동시에 두번째 column이 facecard인 것)의 확률. 분모는 hands의 row number가 되겠군? 1326인가?
#> [1] 0.0483
```
그런데 첫번째 column이 facecard고 두번째 column이 ace인건?

In the last line, we assume the Ace comes first. This is only because we know the way `combination` enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:
아하. combination이 일하는 방식은 ace를 무조건 앞에 두었구만.. 그러니까 ace를 앞에서만 골라도 되었음. ace가 다 첫번째 column에 몰려 있으니 그 안에서ㅓ만 확률 찾아도 되었음. 하지만 확실히 하기 위해 아래와 같이 씀..

```{r}
mean((hands[,1] %in% aces & hands[,2] %in% facecard) |
       (hands[,2] %in% aces & hands[,1] %in% facecard))
#앞이 ace인 것 + 뒤가 ace인 것. 그런데 값은 위와 같음.
```



### 13.6.1 Monte Carlo example

Instead of using `combinations` to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:

```{r}
hand <- sample(deck, 2)
hand
#> [1] "Queen Clubs"  "Seven Spades"
```

And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say _face card_. Now we need to check both possibilities:

```{r}
(hands[1] %in% aces & hands[2] %in% facecard) | 
  (hands[2] %in% aces & hands[1] %in% facecard)
#> [1] FALSE
```
응..? 이게 뭐지..

If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.

Let’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.

```{r}
blackjack <- function(){
   hand <- sample(deck, 2)
  (hand[1] %in% aces & hand[2] %in% facecard) | 
    (hand[2] %in% aces & hand[1] %in% facecard)
}
```
에이스 먼저 뽑거나 나중에 뽑거나 해서 21만든 경우 TRUE.. 계속 뽑는 건가... balckjack 실행할 때마다 계속 sample로 두 개 뽑아서 TRUE 나오는 것 개수 적립.. 세기?
combinations 함수 아니니까 에이스가 먼저 나올 때, 뒤에 나올 때 둘 다 체크해야 함.

Here we do have to check both possibilities: Ace first or Ace second because we are not using the `combinations` function. The function returns `TRUE` if we get a 21 and `FALSE` otherwise:

```{r}
blackjack()
#> [1] FALSE
```
와 실행할 때마다 TRUE나 FALSE 뜨는데 TRUE가 정말 드물게 뜬다.

Now we can play this game, say, 10,000 times:

```{r}
B <- 10000
results <- replicate(B, blackjack())
mean(results)
#> [1] 0.0475
```
실제 확률 느낌? sapply 아니고 replicate..



## 13.7 Examples

In this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.



### 13.7.1 Monty Hall problem

In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?
세 개의 문 있음. 한 개 선택한 참가자. 선택 받지 못한 두 개의 문 중 하나를 열었는데 아무것도 없다. 선택을 바꿀 것이냐?

We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy48 or read one on Wikipedia49.
처음 선택한 문이 답일 확률은 여전히 3분의 1. 그러나 다른 한 개의 문은 확률이 3분의 2로 바뀐다. 왜일까?

Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.

Let’s start with the stick strategy:

```{r}
B <- 10000
monty_hall <- function(strategy){
  doors <- as.character(1:3)
  #문은 3개
  prize <- sample(c("car", "goat", "goat"))
  #각각 위너와 루저를 순서대로 지정.
  prize_door <- doors[prize == "car"]
  #car가 있는 door는 prize_door로 지정
  my_pick  <- sample(doors, 1)
  #선택했다!
  show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)
  #내가 고른 문도 아니고, 진짜 답도 아닌 문을 오픈해라.
  stick <- my_pick
  #내 선택 그대로 간다
  stick == prize_door
  #내 선택이 맞았을까..? (근데 이거 왜 체크?)
  switch <- doors[!doors%in%c(my_pick, show)]
  #보여준 문도 아니고, 내가 골랐던 문도 아닌 문으로 바꾸자.
  choice <- ifelse(strategy == "stick", stick, switch)
  #내 전략에 따라 변경. 전략을 input으로 함수에 입력. stick이면 stick, switch면 switch값 사용.
  choice == prize_door
  #전략에 따라 최종 결정. 내 선택이 맞았을까..?
}
stick <- replicate(B, monty_hall("stick"))
mean(stick)
#> [1] 0.342
switch <- replicate(B, monty_hall("switch"))
mean(switch)
#> [1] 0.668
```

As we write the code, we note that the lines starting with `my_pick` and `show` have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, `show`, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.



### 13.7.2 Birthday problem

Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.

First, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:

```{r}
n <- 50
bdays <- sample(1:365, n, replace = TRUE)
```

To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function `duplicated`, which returns `TRUE` whenever an element of a vector is a duplicate. Here is an example:

```{r}
duplicated(c(1,2,3,1,4,3,5))
#> [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE
```
1이 2번째로 등장한 순간 TRUE 내보냄. (3번째도, 4번째도..)
(3번 이상 등장한 경우는 어떻게 처리하지? 4번 등장해도 도합1로 처리할 순 없는건가..)

The second time 1 and 3 appear, we get a `TRUE`. So to check if two birthdays were the same, we simply use the `any` and `duplicated` functions like this:

```{r}
any(duplicated(bdays))
#> [1] TRUE
```
TRUE가 하나라도 있으면 TRUE라고 내보내나 봄.
any는 로지컬 타입만 input으로 받고 TRUE있을 떄 TRUE 내보내는 로지컬인듯.

In this case, we see that it did happen. At least two people had the same birthday.

To estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:
결국 이건 몇 명이 생일 겹치냐, 몇 개의 생일이 겹치냐를 알아보는 것이 아니라 생일이 겹치는 일이 있냐-를 묻는 거니까.

```{r}
B <- 10000
same_birthday <- function(n){
  bdays <- sample(1:365, n, replace=TRUE)
  any(duplicated(bdays))
}
results <- replicate(B, same_birthday(50))
mean(results)
#> [1] 0.969
```

Were you expecting the probability to be this high?
거의 1인데..?

People tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.

Say we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?
(0.5, 0.75 나올 때까지 size 줄이면?)

Let’s create a look-up table. We can quickly create a function to compute this for any group size:

```{r}
compute_prob <- function(n, B=10000){
  results <- replicate(B, same_birthday(n))
  mean(results)
}
```
사이즈 n에 따라 생일 겹치는 사람 있을 확률 구하기.. 변수는 n.

Using the function `sapply`, we can perform element-wise operations on any function:

```{r}
n <- seq(1,60)
prob <- sapply(n, compute_prob)
```
아아. replicate은 하나의 값인 B를 쭉 이용해서, 단순히 B번 실행하는 거고.
sapply는 특정 수의 범위와 같은 것을 input으로 넣어 1이 input일 때, 2가 input일 때 이런 식으로 여러 번 실행하는 것이구만.

We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size $n$:

```{r}
library(tidyverse)
prob <- sapply(n, compute_prob)
qplot(n, prob)
```

Now let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.

To make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.
A 일 확률보다 not A 일 확률 구하기.

Let’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:

$1×364/365×363/365 … (365−n+1)/365$
(아무도 생일 안 겹칠 확률)
((365−n+1)/365.. 50번째 사람이 아직 안 골랐으니 그 사람이 고를 때 그 확률이 365-n하고 50번째인 것 고려해서 +1이구만?)

We can write a function that does this for any number:

```{r}
exact_prob <- function(n){
  prob_unique <- seq(365,365-n+1)/365 
  1 - prod( prob_unique)
}
eprob <- sapply(n, exact_prob)
#n은 위에서 정의했었지. 1~60.
qplot(n, prob) + geom_line(aes(n, eprob), col = "red")
#한꺼번에 출력.
```

This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities.



## 13.8 Infinity in practice

The theory described here requires repeating experiments over and over forever. In practice we can’t do this. In the examples above, we used $B=10,000$ Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger $B$, the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.

One practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.

```{r}
B <- 10^seq(1, 5, len = 100)
compute_prob <- function(B, n=25){
  same_day <- replicate(B, same_birthday(n))
  mean(same_day)
}
prob <- sapply(B, compute_prob)
qplot(log10(B), prob, geom = "line")
```

In this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.569.



## 13.9 Exercises

1. One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?

```{r}
balls <- c(replicate(3, "cyan"), replicate(5, "magenta"), replicate(7, "yellow"))
mean(balls %in% "cyan" )

3 / (3 + 5 + 7)
```

3 / (3 + 5 + 7) = 0.2
즉 **20%**


2. What is the probability that the ball will not be cyan?

```{r}
mean(!balls %in% "cyan" )
```

1-0.2 = 0.8
즉 **80%**


3. Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling **without** replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?

```{r}
mean(balls %in% "cyan" ) * sum(!balls %in% "cyan")/(length(balls)-1)
```

**0.1714286**


```{r}
balls_3 <- c("cyan 1", "cyan 2", "cyan 3", "magenta 1", "magenta 2", "magenta 3", "magenta 4", "magenta 5", "yellow 1", "yellow 2", "yellow 3", "yellow 4", "yellow 5", "yellow 6", "yellow 7")
balls_4 <- permutations(15, 2, v = balls_3)
first_card <- balls_4[,1]
second_card <- balls_4[,2]

mean(!grepl("cyan", second_card) & grepl("cyan", first_card))
```



4. Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling **with** replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?

```{r}
balls_5<- expand.grid( balls_3, balls_3)
mean(grepl("cyan", balls_5$Var1) & !grepl("cyan", balls_5$Var2))
```

**0.16**

```{r}
3/15 * 12/15
```



5. Tow events A and B are independent if $Pr(A and B) = Pr(A)P(B)$. Under which situation are the draws independent?

a. You don’t replace the draw.
**b**. You replace the draw.
c. Neither
d. Both



6. Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?

**7/15**


7. If you roll a 6-sided die six times, what is the probability of not seeing a 6?

```{r}
(5/6)^6
```
**0.334898**


8. Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game?

Cavs가 이길 확률이 60%. Celtics가 한 번이라도 이긴다는 것은,
1-(한 번도 못 이길 확률) 하면 됨.

```{r}
1-(0.6)^4
```

**0.8704**


9. Create a Monte Carlo simulation to confirm your answer to the previous problem. Use `B <- 10000` simulations. Hint: use the following code to generate the results of the first four games:

Celtic이 이긴 것이 1이고 0.4 확률로 나타난다는 듯.
```{r}
B <- 10000
celtic_wins <- function(){
  celtic_wins <- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))
  any(celtic_wins %in% "1")
}

results <- replicate(B, celtic_wins())
mean(results)
```

The Celtics must win one of these 4 games.

**0.8658**

(앞선 4번의 게임 중 한 번이라도 Celtic이 이긴 빈도)
(1-한 번도 못 이겼을 때와 비교했을 때 둘 다 쉬운 방법인 듯..?)


10. Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?

한 번 지거나 / 두 번 지거나 / 세 번 지거나.
(1)한 번 졌을 경우, 4번은 연달아 이긴다.
(2)두 번 졌을 경우, 처음 진 이후 뒤에 이어진 5번의 게임에서 4번 이기고 한 번 진다. 다섯 번째 게임은 무조건 이겨야 하므로, 앞의 네 번 중 한 번만 지면 된다.
(3)세 번 졌을 경우, 마지막 게임은 꼭 이겨야 하므로 2번째 게임부터 6번째 게임까지 5번의 게임 중 3번 이기면 된다.

(아니 그냥 (1/2)^(게임횟수)*(경우 가짓수)하면 되는 것일까?)
```{r}
#(1)한 번 진 경우 (1가지)
combinations(4, 4, v = c("a", "b", "c", "d"))
a <- (0.5)^4
#(2)두 번 진 경우 (4가지)
combinations(4, 3, v = c("a", "b", "c", "d"))
b <- (0.5)^5 * 4
#(3)세 번 진 경우 (10가지)
combinations(5, 3, v = c("a", "b", "c", "d", "e"))
c <- (0.5)^6 * 10

a + b + c
```

**0.34375**

첫 판을 cav가 지고 난 후
cav가 이긴 경우 - >15가지
warrior가 이긴 경우
(1)한 번도 안 진 경우, 첫 판부터 연달아 이기면 되므로 1가지.
(2)한 번 진 경우, 첫 판 이미 이겼으니 뒤 이은 4판 중 3판을 이기면 된다. 마지막 판은 꼭 이겨야 하므로 3가지.
(3)두 번 진 경우, 뒤 이은 5판 중 3판을 이겨야 한다. 마지막 판은 꼭 이겨야 하므로 6가지.
(4)세 번 진 경우, 뒤 이은 6판 중 3판 이겨야 한다. 마지막 판은 꼭 이겨야 하므로 다섯 판 중 두 판 이기려면... 10가지.

```{r}
combinations(3, 3, v = c("a", "b", "c"))
d <- (0.5)^3
combinations(3, 2, v = c("a", "b", "c"))
e <- (0.5)^4*3
combinations(4, 2, v = c("a", "b", "c", "d"))
f <- (0.5)^5*6
combinations(5, 2, v = c("a", "b", "c", "d", "e"))
g <- (0.5)^6*10
d + e + f + g
```
각 가짓수마다 일어날 확률 다름.
한 게임을 이길 확률은 두 팀 모두 같지만,
n번 진 경우를 따질 때의 가짓수는 n이 어떤 숫자냐에 따라 확률이 다르다.


11. Confirm the results of the previous question with a Monte Carlo simulation.

```{r}
B <- 10000
cav_wins <- function(){
  cav_wins <- sample(c(0,1), 6, replace = TRUE, prob = c(0.5, 0.5))
  ifelse(sum(cav_wins)>=4, TRUE, FALSE)
}

results <- replicate(B, cav_wins())
mean(results)
```

**0.3497**

(이렇게 해도 되는 것일까..?)


12. Two teams, A and B, are playing a seven game series. Team A is better than team B and has a $p>0.5$ chance of winning each game. Given a value $p$, the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:

```{r}
prob_win <- function(p){
  B <- 10000
  result <- replicate(B, {
    b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))
    sum(b_win)>=4
  })
  mean(result)
}
```

Use the function `sapply` to compute the probability, call it `Pr`, of winning for `p <- seq(0.5, 0.95, 0.025)`. Then plot the result.

```{r}
p <- seq(0.5, 0.95, 0.025)
lets_plot <- sapply(p, prob_win)
qplot(p, lets_plot)
```


13. Repeat the exercise above, but now keep the probability fixed at` p <- 0.75` and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, `N <- seq(1, 25, 2)`. Hint: use this function:

```{r}
prob_win <- function(N, p=0.75){
  B <- 10000
  result <- replicate(B, {
    b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
    sum(b_win)>=(N+1)/2
  })
  mean(result)
}
```

```{r}
N <- seq(1, 25, 2)
lets_plot_2 <- sapply(N, prob_win)
qplot(N, lets_plot_2)
```



## 13.10 Continuous probability

In Section 8.4, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size $n$ with extremely high precision, since no two people are exactly the same height, we need to assign the proportion $1/n$ to each observed value and attain no useful summary at all.

Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height.

Just as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the _cumulative distribution function_ (CDF).

We described empirical cumulative distribution function (eCDF) in Section 8.4 as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector $x$ to contain these heights:

```{r}
library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)
```

We defined the empirical distribution function as:

```{r}
F <- function(a) mean(x<=a)
```

which, for any value `a`, gives the proportion of values in the list `x` that are smaller or equal than `a`.
x에 a보다 작거나 같은 게 얼마나 있는지 proportion 보여줌.

Keep in mind that we have not yet introduced probability in the context of CDFs. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing:
학생마다 선택될 가능성 같으므로. 학생 proportion과 확률 같음.

```{r}
1 - F(70)
#> [1] 0.377
```
정규분포 같이 그려진 그 종 모양 그래프 말하는 건가? Pr(x<=a) 뭐 이런? a부터 그 이전까지 density같은..

Once a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height a and height b is:

```{r}
F(b)-F(a)
```

Because we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights `x`.



## 13.11 Theoretical continuous distributions

In Section 8.8 we introduced the normal distribution as a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function `pnorm`. We say that a random quantity is normally distributed with average `m` and standard deviation `s` if its probability distribution is defined by:

```{r}
F(a) = pnorm(a, m, s)
```

This is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:

```{r}
m <- mean(x)
s <- sd(x)
1 - pnorm(70.5, m, s)
#> [1] 0.371
```



### 13.11.1 Theoretical distributions as approximations

The normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:
개별 키를 모두 카테고리화해서 플롯하면.. 매우 discrete.

(그래프)

While most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.001 or 1 in 812. The probability for 70 inches is much higher at 0.106, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787?
69.6850393700787와 70을 다르게 표기하는 것이 정말 유효할까?

Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.
키가 정확히 70인치인 사람은 별로 없으므로 연속적 데이터로 사용하는 것이 좋다.

With continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.
범위로 확률을 묻는다. 개별 키의 확률 묻지 않고.

In cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:

```{r}
mean(x <= 68.5) - mean(x <= 67.5)
#> [1] 0.115
mean(x <= 69.5) - mean(x <= 68.5)
#> [1] 0.119
mean(x <= 70.5) - mean(x <= 69.5)
#> [1] 0.122
```
이런 학생들이 몇% 있는가.

Note how close we get with the normal approximation:

```{r}
pnorm(68.5, m, s) - pnorm(67.5, m, s) 
#> [1] 0.103
pnorm(69.5, m, s) - pnorm(68.5, m, s) 
#> [1] 0.11
pnorm(70.5, m, s) - pnorm(69.5, m, s) 
#> [1] 0.108
```
정규분포(?) 써도 위와 매우 비슷.

However, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:
그런데 어떤 interval 확률 다룰 때는 오류가 크다.

```{r}
mean(x <= 70.9) - mean(x<=70.1)
#> [1] 0.0222
```

with

```{r}
pnorm(70.9, m, s) - pnorm(70.1, m, s)
#> [1] 0.0836
```

In general, we call this situation _discretization_. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.
수가 이산적으로 변할 때 얘기하는 듯?
현실이랑 잘 비교해야 함. 현실에서는 연속적으로 데이터가 변하지 않으므로.



### 13.11.2 The probability density

For categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it $X$, can be 1,2,3,4,5 or 6. The probability of 4 is defined as:

$Pr(X=4)=1/6$

The CDF can then easily be defined:

$F(4)=Pr(X≤4)=Pr(X=4)+Pr(X=3)+Pr(X=2)+Pr(X=1)$

Although for continuous distributions the probability of a single value $Pr(X=x)$ is not defined, there is a theoretical definition that has a similar interpretation. The probability density at $x$ is defined as the function $f(a)$ such that:

$F(a)=Pr(X≤a)=∫_(-∞)^a(f(x)dx)$

적분.
For those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of  $f(x)$ as a curve for which the area under that curve up to the value $a$, gives you the probability $Pr(X≤a)$.

For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:

```{r}
1 - pnorm(76, m, s)
dnorm(76, m, s)
#> [1] 0.0321
```
(뭐지..?)

which mathematically is the grey area below:

(그래프)

The curve you see is the probability density for the normal distribution. In R, we get this using the function `dnorm`.
음.. density 플롯해준다는 것인가 아니면 1 마이너스를 자동적으로 한다는 것인가. 뭐여?

Although it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available.



## 13.12 Monte Carlo simulations for continuous variables

R provides functions to generate normally distributed outcomes. Specifically, the `rnorm` function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:

```{r}
n <- length(x)
m <- mean(x)
s <- sd(x)
simulated_heights <- rnorm(n, m, s)
```
합해서 사이즈 n되는 데이터를 랜덤하게 생성한다는 것인 듯. 그런데 정규분포 종모양 닮은 것으로.

Not surprisingly, the distribution looks normal:

(그래프)

This is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.
이렇게 했을 때 어떤 모양 나타날 수 있냐 추측 정도 느낌인가?

If, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:
내가 다른 데이터에서 얻은 키 평균과 sd 정보를 이용해, 가상의 800명의 데이터를 얻었다고 생각하면 그 800명 분포는 대략 어떻게 나타날까?

```{r}
B <- 10000
tallest <- replicate(B, {
  simulated_data <- rnorm(800, m, s)
  max(simulated_data)
})
```
그러니까 이 tallest는, 평균과 sd 정보를 이용해 가상의 800명 데이터를 얻은 후 이 800명 제일 키 큰 사람의 키 정보를 tallest에 입력. 그런데 이렇게 1.데이터 얻고 2.가장 큰 키 정보 입력 ~이걸 B번 반복한다는 것이지?

Having a seven footer is quite rare:

```{r}
mean(tallest >= 7*12)
#> [1] 0.0184
```
가장 키 큰 사람 중 7*12와 같거나 보다 큰 사람은 몇 퍼센트냐. 가장 키 큰 사람 중에서도 얼마 안 되는군.

Here is the resulting distribution:

(그래프)

Note that it does not look normal.
이런 것도 의미 있는 데이터인가? 랜덤하게 얻은 최대 키 정보만 모아 놓는 것이 유효한 데이터임?



## 13.13 Continuous distributions

We introduced the normal distribution in Section 8.8 and used it as an example above. The normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters `d`, `q`, `p`, and `r` in front of a shorthand for the distribution. We have already seen the functions `dnorm`, `pnorm`, and `rnorm` for the normal distribution. The functions `qnorm` gives us the quantiles. We can therefore draw a distribution like this:

```{r}
x <- seq(-4, 4, length.out = 100)
qplot(x, f, geom = "line", data = data.frame(x, f = dnorm(x)))
```
(그래서 qnorm 설명은..?)

For the student-t, described later in Section 16.10, the shorthand `t` is used so the functions are `dt` for the density, `qt` for the quantiles, `pt` for the cumulative distribution function, and `rt` for Monte Carlo simulation.



## 13.14 Exercises

1. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?

```{r}
pnorm(60, 64, 3) *100
```

**9.121122%**


2. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?

```{r}
(1 - pnorm(72, 64, 3))*100
```

**0.3830381%**


3. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?

```{r}
(pnorm(67, 64, 3) - pnorm(61, 64, 3))*100
```

**68.26895%**


4. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?

```{r}
(pnorm(67*2.54, 64*2.54, 3*2.54) - pnorm(61*2.54, 64*2.54, 3*2.54))*100
```

**68.26895%**

_**same.**_


5. Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.
(64가 아니라 67같은데..?)

```{r}
(pnorm(m + s, m, s) - pnorm(m - s, m, s))*100
```

**68.26895%**

어떻게 하는 거지...?



6. To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average $m$ and standard error $s$. Suppose we ask the probability of $X$ being smaller or equal to $a$. Remember that, by definition, $a$ is $(a−m)/s$ standard deviations $s$ away from the average $m$. The probability is:

$Pr(X ≤ a)$

Now we subtract $μ$ to both sides and then divide both sides by $σ$ :

$Pr((X-m)/s ≤ (a-m)/s)$

The quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it $Z$ :

$Pr(Z) ≤ (a-m)/s$

So, no matter the units, the probability of $X ≤ a$ is the same as the probability of a standard normal variable being less than $(a-m)/s$. If `mu` is the average and `sigma` the standard error, which of the following R code would give us the right answer in every situation:

**a**. `mean(X<=a)`
b. `pnorm((a - m)/s)`
c. `pnorm((a - m)/s, m, s)`
d. `pnorm(a)`

```{r}
pnorm((64 - m)/s, 0, 1)
```

(???)


7. Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use `qnorm`.

```{r}
qnorm(0.99, 69, 3)
```

**75.97904 inches**

(?)


8. The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with `B=1000` generating 10,000 IQ scores and keeping the highest. Make a histogram.


```{r}
B <- 1000

highest <- function(){
  this_highest <-rnorm(10000, 100, 15)
  max(this_highest)
}
whats_highest <- data.frame(height = replicate(B, highest()))

ggplot(whats_highest, aes(x = height)) + geom_histogram(binwidth = 1)

```


```{r}
B <-1000
highest_2 <- replicate(B, {
  simulated_heights_ <-rnorm(10000, 100, 15)
  max(simulated_heights_)
})

highest <- data.frame(height = highest_2)

ggplot(highest, aes(x = height)) + geom_histogram(binwidth = 1)
```




done!